[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "impactfulai",
    "section": "",
    "text": "Stable Diffusion Sampling Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "",
    "text": "One of the great things with Diffusion Models (DMs) is that we can tune a bit their results without retraining them. Indeed, their inference process is somewhat an optimization process, and therefore can be tuned at several levels.\nIn this notebook, I try to analyze a bit the sampling process of Stable Diffusion, and derive from this analysis potential small variations (improvements?) of the images generated."
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#imports-and-utils",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#imports-and-utils",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Imports And Utils",
    "text": "Imports And Utils\nTo run Stable Diffusion on your computer you have to accept the model license. Itâ€™s an open CreativeML OpenRail-M license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The model card provides more details. If you do accept the license, you need to be a registered user in ðŸ¤— Hugging Face Hub and use an access token for the code to work. You have two options to provide your access token:\n\nUse the huggingface-cli login command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer.\nOr use notebook_login() in a notebook, which does the same thing.\n\n\n# !pip install -Uq diffusers transformers fastcore\n\n\nimport torch\ntorch.cuda.set_device(1) \n\n/home/seb.derhy/anaconda3/envs/sdv2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nfrom PIL import Image\nfrom fastcore.all import concat\nimport torch, logging\nfrom pathlib import Path\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nlogging.disable(logging.WARNING)\n\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\n\nfrom tqdm.auto import tqdm\n\nVery useful util function to display a grid of images\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid"
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#using-stable-diffusion",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#using-stable-diffusion",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Using Stable Diffusion",
    "text": "Using Stable Diffusion\n\nseed = 123\n\n\n## From: https://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts/ \nprompts = [\n    'portrait photo of an astronaut riding a horse',\n    'portrait photo of a handsome businesssman',\n    'portrait photo of an asia old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes, 50mm portrait photography, hard rim lighting photography',\n    'portrait photo headshot by mucha, sharp focus, elegant, render, octane, detailed, award winning photography, masterpiece, rim lit'\n]\n\n\nguidance_scale = 7.5\nnum_inference_steps = 50\ndevice = torch.device(\"cuda\")\nnum_images_per_prompt = 1\n\n\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nFetching 12 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 4237.38it/s]\n\n\nLetâ€™s start by running the pipeline â€œoff-the-shelfâ€, without any modification\n\ntorch.manual_seed(seed)\nimages = pipe(prompts, \n             guidance_scale=guidance_scale, \n             num_inference_steps=num_inference_steps, \n             num_images_per_prompt=num_images_per_prompt).images\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:55<00:00,  1.11s/it]\n\n\n\nlen(images)\n\n4\n\n\n\nref_images = images.copy()\n\n\nimage_grid(ref_images, rows=num_images_per_prompt, cols=len(prompts))"
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#log-the-noise-predictions",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#log-the-noise-predictions",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Log the noise predictions",
    "text": "Log the noise predictions\n\ndef update_with_debug_logs(latents, i, t, text_embeddings, do_classifier_free_guidance):\n    # expand the latents if we are doing classifier free guidance\n    latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    if do_classifier_free_guidance:\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_unconds.append(noise_pred_uncond.detach().cpu()) ## Log the noise unconds results\n        noise_conds.append(noise_pred_text.detach().cpu()) ## Log the noise conds results\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n        noise_preds.append(noise_pred.detach().cpu())\n\n    # compute the previous noisy sample x_t -> x_t-1\n    new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n    latent_step = new_latents-latents \n\n    return new_latents\n\n\nnoise_preds, noise_unconds, noise_conds = [], [], []\ntorch.manual_seed(seed)\nimgs = my_sd_sampling(custom_update=update_with_debug_logs)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:55<00:00,  1.12s/it]\n\n\n\nimport numpy as np\nassert np.sum(np.array(imgs[0])-np.array(ref_images[0]))==0\n\n\n# image_grid(ref_images[:len(prompts)] + imgs[:len(prompts)] \n#            + ref_images[len(prompts):] + imgs[len(prompts):], rows=2*num_images_per_prompt, cols=len(prompts))"
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#compare-the-noise-norms",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#compare-the-noise-norms",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Compare the noise norms",
    "text": "Compare the noise norms\nOne thing we observe from the code above (noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)) is that we actually pass the input image twice to the network at each iteration:\n\nOnce without the conditional text input -> Predict the noise from the noisy image only\nOnce with the conditional text input -> Predict the noise from the noisy image AND the image descriptionâ€™s encoding\n\nSince these inputs are a bit different, we expect these 2 passes to predict different noise values (i.e.Â point towards different real images). However, since both networks are trained to predict noise from a noisy image, we should expect the noise prediction to have approximately the same norm in both cases.\nLetâ€™s see if this is true, and letâ€™s also compare this norm with the final noise prediction norm.\n\nnoise_preds_norm = [pred.norm() for pred in noise_preds]\nnoise_unconds_norm = [pred.norm() for pred in noise_unconds]\nnoise_conds_norm = [pred.norm() for pred in noise_conds]\n\n\nplt.scatter(range(num_inference_steps), noise_unconds_norm, c='b', alpha=0.5, label=\"Unconditional noise pred norm\")\nplt.scatter(range(num_inference_steps), noise_conds_norm, c='g', alpha=0.5, label=\"Conditional noise pred norm\")\nplt.scatter(range(num_inference_steps), noise_preds_norm, c='r', alpha=0.5, label=\"Final noise pred norm\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f5ab3fd3670>\n\n\n\n\n\nHere we indeed see that the unconditional and conditional noise norms are approximately the same. However, the final noise prediction seems to have a slightly different norm? Why is that?\nThe answer to this question is in the next line of code:\nnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\nThis line does a weighted average between the conditioned and unconditioned noise prediction, and has therefore ABSOLUTELY NO REASON to have the same norm.\nDoes this make sense? Wellâ€¦ In my opinion, not so much, so letâ€™s try to re-normalize things :)."
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#log-trajectory",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#log-trajectory",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Log trajectory",
    "text": "Log trajectory\nBelow, we are going to log the trajectory point at each timestep, so that we can analyze a bit whatâ€™s happening\n\ndef update_with_debug_logs(latents, i, t, text_embeddings, do_classifier_free_guidance):\n    # expand the latents if we are doing classifier free guidance\n    latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    if do_classifier_free_guidance:\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n    # compute the previous noisy sample x_t -> x_t-1\n    new_latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n    latent_step = new_latents-latents \n\n    # THIS IS THE IMPORTANT LINES --> Log information\n    traj_pts_times.append(t.detach().cpu())\n    traj_pts_values.append(new_latents.detach().cpu())\n    return new_latents\n\n\ntraj_pts_times, traj_pts_values = [], []\ntorch.manual_seed(seed)\nimgs = my_sd_sampling(custom_update=update_with_debug_logs)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:55<00:00,  1.12s/it]\n\n\nI find the fact that time is moving backward a bit unintuitive, especially for the kind of things weâ€™ll do next, so letâ€™s reverse it.\n\ntraj_pts_times[:10]\n\n[tensor(999., dtype=torch.float64),\n tensor(978.6122, dtype=torch.float64),\n tensor(958.2245, dtype=torch.float64),\n tensor(937.8367, dtype=torch.float64),\n tensor(917.4490, dtype=torch.float64),\n tensor(897.0612, dtype=torch.float64),\n tensor(876.6735, dtype=torch.float64),\n tensor(856.2857, dtype=torch.float64),\n tensor(835.8980, dtype=torch.float64),\n tensor(815.5102, dtype=torch.float64)]\n\n\n\ntraj_pts_times = [999-t for t in traj_pts_times]"
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#visualize-the-trajectory-using-dimension-reduction",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#visualize-the-trajectory-using-dimension-reduction",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Visualize the trajectory using dimension reduction",
    "text": "Visualize the trajectory using dimension reduction\nHere we will perform 3 different types of dimension reduction in order to visualize the trajectory - T-SNE - PCA - MDS\nIn these plot, yellow represent the enf of the sampling process, while the purple represents the beginning of the sampling process.\n\nfrom sklearn.manifold import TSNE\nX = torch.stack(traj_pts_values).view(len(traj_pts_values), -1)\nX.shape\n\ntorch.Size([50, 65536])\n\n\n\nX_lowdim = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(X)\nX_lowdim.shape\n\n(50, 2)\n\n\n\nplt.scatter(X_lowdim[:, 0], X_lowdim[:, 1], c=traj_pts_times)\n\n<matplotlib.collections.PathCollection at 0x7f5ab3eca730>\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_lowdim = pca.fit_transform(X)\nX_lowdim.shape\n\n(50, 2)\n\n\n\nplt.scatter(X_lowdim[:, 0], X_lowdim[:, 1], c=traj_pts_times)\n\n<matplotlib.collections.PathCollection at 0x7f5aaaaf7fa0>\n\n\n\n\n\nI personally really like the MDS representation, because it tries to project the data in a way that best preserves distances.\n\nfrom sklearn.manifold import MDS\nmds = MDS()\nX_lowdim = mds.fit_transform(X)\nX_lowdim.shape\n\n(50, 2)\n\n\n\nplt.scatter(X_lowdim[:, 0], X_lowdim[:, 1], c=traj_pts_times)\n\n<matplotlib.collections.PathCollection at 0x7f5aaaa729d0>\n\n\n\n\n\nWe can see that the trajectory is quite smooth!! This is something weâ€™ll try to leverage later on. But first letâ€™s do some more analysis:"
  },
  {
    "objectID": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#analyze-the-steps-length-and-directions",
    "href": "posts/2022-25-12-stable-diffusion-sampling-analysis.html#analyze-the-steps-length-and-directions",
    "title": "Stable Diffusion Sampling Experiments",
    "section": "Analyze the steps length and directions",
    "text": "Analyze the steps length and directions\nOne interesting way to analyze the trajectory is to look at each step performed in the sampling loop: - How big was the step? - How much is it changing direction in the course of sampling?\n\nsteps = [(traj_pts_values[i+1]-traj_pts_values[i]) for i in range(len(traj_pts_values)-1)]\nsteps_norms = [step.norm() for step in steps]\nsteps_unit_vecs = [step/step.norm() for step in steps]\nsteps_dir_change = [(steps_unit_vecs[i+1]-steps_unit_vecs[i]).norm() for i in range(len(steps_unit_vecs)-1)]\n\n\nfig, axs = plt.subplots(2, 1, figsize=(18, 12))\naxs[0].scatter(traj_pts_times[2:], steps_dir_change)\naxs[0].set_title(\"Direction Changes Strength\")\naxs[1].scatter(traj_pts_times[1:], steps_norms)\naxs[1].set_title(\"Step Length\")\n\nText(0.5, 1.0, 'Step Length')\n\n\n\n\n\nInterestingly, we see that the direction is changing a lot at the end. This tends to make me think that the algorithm overshoots somehow at the end. Letâ€™s try to fix this!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]